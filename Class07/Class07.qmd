---
title: "Class 7: Machine Learning 1"
author: "Arshiya Zarmahd (PID: A16247996)"
format: pdf
---

Today we will start our multi-part exploration of some key machine learning methods. We will begin with clustering -- finding groupings in data, and then dimensionallity reduction. 

## Clustering

Let's start with "k-means" clustering. 
The main function in base R for this is `k-means()`

```{r}
hist( rnorm(100000, mean=3) )
```

```{r}
tmp <- c(rnorm(30, -3), rnorm(30, +3))
x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```

Now lets try `kmeans()`

```{r}
km <- kmeans(x, centers=2)
km
```
> Q. How many points in each cluster?

```{r}
km$size
```

> Q. What component of your result object details cluster assignment/membership?

```{r}
km$cluster
```

> Q. What are centers/mean valuses of each cluster?

```{r}
km$centers
```

> Q. Make a plot of your data showing your clustering results (groupings/clusters and cluster centers). 

```{r}
plot(x, col=c("red", "blue") )
```

```{r}
c(1:5) + c(100, 1)
```

```{r}
plot(x, col= c(1,2))
```

```{r}
plot(x, col=km$cluster)
points(km$centers, col="green", pch=15, cex=3)
```

> Q. Run `kmeans()` again and cluster in 4 groups and plot the results. 

```{r}
km4 <- kmeans(x, centers = 4)
km4
plot(x, col=km4$cluster)
```

## Hierarchical Clustering

This form of clustering aims to reveal the structure in your data by progressively grouping points into an ever smaller number of clusters. 

The main function in base R for this is called `hclust()`.This function does not take our input data directly, but wants a "distance matrix" that details how (dis)similar all our input points are to each other. 

```{r}
hc <- hclust( dist(x) )
hc
```

The print out above is not very useful (unlike that of kmeans), but there is a useful `plot()` method. 

```{r}
plot(hc)
```

To get my main result (my cluster membership vector), I need to "cut" my tree using the function `cutree()`.

```{r}
grps <- cutree(hc, h=10)
grps
```

```{r}
plot(x, col=grps)
```

```{r}
plot(x, col=cutree(hc, h=6))
```

## Part 1: PCA of UK Food Data

# Data Import

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
x
```

# Checking Your Data

> Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
## Complete the following code to find out how many rows and columns are in x?
dim(x)
nrow(x)
ncol(x)
```

There are 17 rows, and 5 columns. But one column is counting the name of the foods; we only expect 4 columns for each country.   

```{r}
## Preview the first 6 rows
head(x)
```

```{r}
x <- read.csv(url, row.names=1)
head(x)
```

```{r}
dim(x)
```

> Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

The `rownames()` function did not work for me after running it more than once. I ran the code several times, but the code began to render a result along the lines of "incorrect dimensions, cannot produce this result". The `read.csv()` function immediately worked and rendered the correct dimensions after using the `dim(x)` code. 

# Spotting Major Differences and Trends

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

> Q3. Changing what optional argument in the above `barplot()` function results in the following plot?

```{r}
barplot(as.matrix(x), col=rainbow(nrow(x)))
```

By setting `besides` to FALSE, the bar plot changed to a horizontally stacked rather than vertically stacked plot. `Besides` is a logical value and when not mentioned, the bar plot will default to a horizontally stacked configuration. 

> Q5. Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

```{r}
pairs(x, col=rainbow(10), pch=16)
```

The pairs plot is useful for small datasets but it can be lots of work to interpret and gets untractable for larger datasets. 

So PCA to the rescue...

The main function to do PCA in base R is called `prompt()`. This function wants to transpose our data in this set. 

```{r}
pca <- prcomp(t(x))
summary(pca)
```

```{r}
attributes(pca)
```

```{r}
pca$x
```

A major PCA result is called a "PCA Plot". 

```{r}
mycols <- c("orange", "red", "blue", "darkgreen")
plot(pca$x[,1], pca$x[,2], col=mycols, pch=16,
     xlab="PC1", ylab="PC2")
abline(h=0, col="gray")
abline(v=0, col="gray")
```

Another important output from PCA is called the "Loadings" vector or the "Rotations" component. This tells us how much the original variables (foods in this case) contribute to the new PCs. 

```{r}
pca$rotation
```

PCA looks to be a super useful method for gaining some insight into high dimensional data that is difficult to examine in other ways. 

```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
head(rna.data)
```





















